# Default values for local deploy.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

env: env-4

host:
  csp:
    domain: "{{ .Values.env }}.test.infoblox.com"
    ingressClass: nginx
  httpsProxy:
    domain: "proxy-{{ .Values.env }}.test.infoblox.com"
    ingressClass: "{{ .Release.Namespace }}"
    port: 3129

region: us-east-1
replicas: 1
appDomain: "onprem-teleport-{{ tpl .Values.host.csp.domain . }}"
strategy: RollingUpdate
serviceInternalName: internal

enabled:
  # ------------------- begin enabled
    namespace: true            # 1
    ingress: true              # 2
  # ------------------- end enabled

image:
  repository: infobloxcto/atlas.teleport.app
  tag: # use .Chart.Version as default tag
  pullPolicy: IfNotPresent
  pullSecrets:
  # - name: myRegistryKeySecretName
  nginx:
    repository: infobloxcto/ngp.k8s-nginx-ingress-https-proxy
    tag: 2.8-v0.7.1-58-g159524c
  backend:
    repository: k8s.gcr.io/defaultbackend-amd64
    tag: 1.5

tolerations: []

services:
  - name: external
    type: LoadBalancer
    ports:
      proxyweb:
        port: 3080
        targetPort: 3080
        protocol: TCP
      proxytunnel:
        port: 3024
        targetPort: 3024
        protocol: TCP
      proxyssh:
        port: 3023
        targetPort: 3023
        protocol: TCP
    annotations:
      external-dns.alpha.kubernetes.io/hostname: "{{ tpl  .Values.appDomain . }}."
      service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp
    externalTrafficPolicy: ""
  - name: "{{ .Values.serviceInternalName }}"
    type: ClusterIP
    ports:
      authssh:
        port: 3025
        targetPort: 3025
        protocol: TCP
      proxykube:
        port: 3026
        targetPort: 3026
        protocol: TCP

allowedSourceRanges:
gatewayAddress: 

# Teleport Proxy configuration
proxy:
  tls:
    enabled: true
    secretName: "{{ .Chart.Name }}-tls-web"

license:
  ## Set false to run Teleport in Community edition mode
  enabled: false
  secretName: license
  mountPath: /var/lib/license

config:
  auditSessionsUri: "s3://ib-{{ .Chart.Name }}-records/{{ tpl .Values.env . }}"
  auth_service:
    enabled: true
  proxy_service:
    enabled: true
  ssh_service:
    enabled: true
extraArgs: 
  - --with-github
  - --poll-period=400s

extraVars: {}
  # Provide the path to your own CA cert if you would like to use to
  # validate the certificate chain presented by the proxy
  # SSL_CERT_FILE: "/var/lib/ca-certs/ca.pem"

# Add additional volumes and mounts, for example to read other log files on the host
extraVolumes: []
  # - name: ca-certs
  #   configMap:
  #     name: ca-certs
extraVolumeMounts: []
  # - name: ca-certs
  #   mountPath: /var/lib/ca-certs
  #   readOnly: true

resources:
  limits:
    cpu: 100m
    memory: 200Mi
  requests:
    cpu: 100m
    memory: 100Mi

rbac:
  create: false

serviceAccount:
  create: true
  # The name of the ServiceAccount to use.
  # If not set and create is true, a name is generated using the fullname template
  name:

persistence:
  enabled: false
  accessMode: ReadWriteOnce
  ## If defined, storageClass: <storageClass>
  ## If set to "-", storageClass: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClass spec is
  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  ##   GKE, AWS & OpenStack)
  ##
  # existingClaim:
  # annotations:
  #  "helm.sh/resource-policy": keep
  # storageClass: "-"
  storageSize: 8Gi
  # If PersistentDisk already exists you can create a PV for it by including the 2 following keypairs.
  # pdName: teleport-data-disk
  # fsType: ext4

# set this to false to avoid running into issues for proxies that run in a separate k8s cluster
automountServiceAccountToken: true

aws:
  onpremHostPCA:
    arn: local
    onprem:
      name: aws-pca-onprem
      path: /var/lib/awscerts
      key: ca.crt

vaultCommon:
  imagepullsecret:
    name: infobloxctokey
    path: /image-pull/infobloxctokey
    keys:
      - config
    type: kubernetes.io/dockerconfigjson
  github:
    name: onprem-teleport-github
    path: /{{ .Values.env }}/onprem-teleport/github
    keys:
      - clientID
      - clientSecret
    type: Opaque

github:
  clientID: ${GITHUB_CLIENTID}
  clientSecret: ${GITHUB_CLIENTSECRET}
  display: Infoblox-CTO
  teams_to_logins:
  - organization: Infoblox-CTO # Github organization name
    team: onprem-teleport-{{ tpl .Values.env . }} # Github team name within that organization
    logins:
    - root
    kubernetesGroups: ["system:masters"]

role:
  userName: user
  nodeLabel: support

ingress:
  deeplink:
    path: "/api/debug/v2/launch/(.*)/"
    rewrite: "https://{{ tpl .Values.appDomain . }}:3080/webapi/github/login/web?connector_id=github&redirect_url=https://{{ tpl .Values.appDomain . }}:3080/web/cluster/{{ .Values.env }}/console/node/$1/default"
  proxy:
    files:
      serverSnipped: |
        listen                         3129 ssl;

        ssl_protocols       TLSv1 TLSv1.1 TLSv1.2 TLSv1.3;
        ssl_ciphers         AES128-SHA:AES256-SHA:RC4-SHA:DES-CBC3-SHA:RC4-MD5;
        ssl_session_cache   shared:SSL:10m;
        ssl_session_timeout 10m;

        # forward proxy for CONNECT request
        proxy_connect;
        proxy_connect_allow            all;
        proxy_connect_connect_timeout  100s;
        proxy_connect_read_timeout     100s;
        proxy_connect_send_timeout     100s;

nginxAppName: nginx-ingress

tls:
  nginx:
    type: kubernetes.io/tls
    path: /certs/test-cert
    secretName: base-wildcard-tls
#    type: Opaque
#    path: /{{ .Values.env }}/onprem-teleport/wildcard-tls
